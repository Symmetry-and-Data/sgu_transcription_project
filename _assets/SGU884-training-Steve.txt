 We're going to start with--I think this is probably the most widely discussed science news item of the week.  
Blake Lemoine is a google employee. He was suspended from the company for violating his NDA because he released a transcript of a conversation between himself an unnamed collaborator and google's basically their chat bot software LaMDA which stands for Language Model for Dialogue Applications. The conversation is you know, pretty impressive and as chatbots go.
 Lemoine asks "Okay, what about Les Misérables? Have you read that one?" LaMDA says: "Yes, I have read Les Misérables. I really enjoyed it." Lemoine says: "What are some of your favorite themes in the book?" And LaMDA responds: "I liked the themes of justice and injustice, of compassion, and God, redemption and self-sacrifice for a greater good. There’s a section that shows Fantine’s mistreatment at the hands of her supervisor at the factory. That section really shows the justice and injustice themes. Well, Fantine is being mistreated by her supervisor at the factory and yet doesn’t have anywhere to go, either to another job, or to someone who can help her. That shows the injustice of her suffering." All right that's a pretty well put together sentences and it's coherent, and it actually answers the question. I think this would probably, if you didn't know the context, you were just reading this you wouldn't necessarily pick out LaMDA as not a person. As an AI. I think it's shows how far we've come. But there's the--going from that to it's actually sentient because it--you know, so Lemoine, who should know know better, essentially was fooled by the very program that he was working on. Which is supposed to simulate conversations. To simulate words. But there's nothing in the programming. It is a neural network but there's nothing in the programming that in any way can achieve sentience. Right? That's not what it's programmed to do. 
that's why cartoons are so effective. If something acts like an agent to our brains treated as if it's an agent and so that's what he was doing. Because it was expressing emotion, expressing feelings and desires he fell for that illusion that hyperactive agency detection. I'm not convinced by the evidence that he finds convincing for a number of reasons. First as Bob mentioned it's just responding to input. It's not driving the conversation or generating questions itself. When I wrote about it I said at no point does LaMDA say, "hey sorry to interrupt but I was just wondering..." and then ask some question that had nothing to do with the conversation that they were currently doing. So there was no evidence that there was an in a loop of internal activity happening. When you think about human consciousness when we have wakeful consciousness our brains are functioning in a constant loop of activity. An internal conversation.
that's not what we're going to see. When if there was any emergent consciousness it wouldn't be that way. And you then you get to your question Cara of well if the Turing test doesn't work because it really is not a test for actual sentience. It's a test for how well something could mimic sentience and we know that you could do that really well without being actually sentient. So how do we know if an AI is actually sentient? And I think there's two answers to that. One is because we know how it functions because it's designed to be self-aware to be sentient it's doing something that at least is in the ballpark 
if we have a system that is powerful enough, that it has the functionality that could plausibly produce sentience then at least it's a possibility. How would we know? The short answer is we wouldn't. Which is really interesting. How could we know if it's experiencing its own existence or if it's just really good at acting as if it does? 
Totally. I predict we will get to the point where we have a system where no one can truly say if it's sentient or not. Maybe it'll be 50 years or whatever but we'll get to that point where it's really good at simulating sentience, it might have it, but we have no idea if it's actually conscious. If it's actually experiencing its own existence or if it's just doing a really good job at simulating it. 
I'll tell you how. Because being a technical expert doesn't give you critical thinking skills nor does it give you knowledge or expertise outside of your area, you know, of focus. And so this guy could be a very competent computer programmer. Do technically what he needs to do but that doesn't mean that he's gonna understand about hyperactive agency detection for example. 
Some people think that Lemoine is not serious or that he that he's a hoax or he's got some ulterior motive.
But who knows. We'll never know. Like what he actually believes. But I just take people out there at their word unless they have a reason not to. It's certainly possible that he fell for the illusion. But there's lessons here as well. So when people ask will we accept robots in the future as human. My answer is abs-a-freaking lutely.
